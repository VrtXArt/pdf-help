import os
import json
import docx
import faiss
from langdetect import detect
from sentence_transformers import SentenceTransformer
import argostranslate.package
import argostranslate.translate

# Inicializa o modelo de embeddings e FAISS
modelo = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')
index = faiss.IndexFlatL2(384)

# Pasta onde est√£o os .docx
PASTA_DOCS = "livros/"
MAPEAMENTO_ARQUIVO = "mapeamento.json"

# Fun√ß√£o para traduzir texto usando Argos Translate
def traduzir_texto(texto, idioma_origem="en", idioma_destino="pt"):
    if not texto.strip():
        return "Texto vazio ou inv√°lido."

    try:
        # Obt√©m os idiomas instalados
        idiomas = argostranslate.translate.get_installed_languages()
        idioma_origem_obj = next((x for x in idiomas if x.code == idioma_origem), None)
        idioma_destino_obj = next((x for x in idiomas if x.code == idioma_destino), None)

        # Verifica se os objetos de idioma foram encontrados
        if not idioma_origem_obj or not idioma_destino_obj:
            return f"Erro: N√£o foi poss√≠vel encontrar os idiomas {idioma_origem} ‚Üí {idioma_destino}. Verifique se os pacotes est√£o instalados corretamente."

        # Executa a tradu√ß√£o usando get_translation()
        traducao = idioma_origem_obj.get_translation(idioma_destino_obj).translate(texto)
        return traducao

    except Exception as e:
        return f"Erro ao traduzir: {e}"



# Fun√ß√£o para processar os documentos .docx e indexar por par√°grafo
def processar_docs_por_paragrafo():
    mapeamento = {}
    contador = 0

    for docx_file in os.listdir(PASTA_DOCS):
        if docx_file.endswith(".docx"):
            caminho_docx = os.path.join(PASTA_DOCS, docx_file)
            print(f"üìÑ Tentando processar o documento: {docx_file}")

            try:
                doc = docx.Document(caminho_docx)
                encontrou_paragrafo = False

                for i, paragrafo in enumerate(doc.paragraphs):
                    texto = paragrafo.text.strip()

                    # Ignorar par√°grafos vazios ou muito curtos
                    if not texto or len(texto) < 5:
                        continue

                    encontrou_paragrafo = True
                    vetor = modelo.encode([texto])[0]
                    index.add(vetor.reshape(1, -1))

                    # Armazena o par√°grafo e suas informa√ß√µes no mapeamento
                    mapeamento[contador] = {
                        "docx": docx_file,
                        "paragrafo": i + 1,
                        "texto": texto
                    }
                    contador += 1

                if not encontrou_paragrafo:
                    print(f"‚ö†Ô∏è Aviso: Nenhum par√°grafo v√°lido encontrado em {docx_file}")

            except Exception as e:
                print(f"‚ùå Erro ao processar o documento {docx_file}: {e}")

    # Salva o mapeamento em JSON
    with open(MAPEAMENTO_ARQUIVO, 'w') as f:
        json.dump(mapeamento, f)
    print("üìÇ Processamento conclu√≠do e mapeamento salvo.")


# Fun√ß√£o para buscar par√°grafos relacionados e traduzir para portugu√™s, se necess√°rio
def buscar_trechos_semanticos(frase, top_k=10):
    # Detecta o idioma da frase
    idioma_busca = detect(frase)

    # Se a frase estiver em portugu√™s, traduz para ingl√™s
    if idioma_busca == "pt":
        frase = traduzir_texto(frase, "pt", "en")

    # Gera o vetor da frase de busca
    vetor_busca = modelo.encode([frase])[0].reshape(1, -1)
    _, indices = index.search(vetor_busca, top_k)

    # Carrega o mapeamento
    with open(MAPEAMENTO_ARQUIVO, 'r') as f:
        mapeamento = json.load(f)

    # Coleta os resultados e os organiza por documento
    resultados_por_doc = {}
    for idx in indices[0]:
        if str(idx) in mapeamento:
            resultado = mapeamento[str(idx)]
            docx_file = resultado["docx"]
            texto_paragrafo = resultado["texto"]

            # Traduz o par√°grafo para portugu√™s, se necess√°rio
            if detect(texto_paragrafo) != "pt":
                texto_paragrafo = traduzir_texto(texto_paragrafo, "en", "pt")

            if docx_file not in resultados_por_doc:
                resultados_por_doc[docx_file] = []

            resultados_por_doc[docx_file].append(texto_paragrafo)

    # Limita os resultados a at√© 3 par√°grafos por documento
    resultados_finais = {doc: parags[:3] for doc, parags in resultados_por_doc.items()}

    return resultados_finais

# Processo principal
if __name__ == "__main__":
    print("üîÑ Processando documentos .docx...")
    processar_docs_por_paragrafo()

    frase_busca = input("Digite a frase em portugu√™s para buscar: ")
    resultados = buscar_trechos_semanticos(frase_busca)

    print("\nüìå Resultados encontrados:")
    for doc, parags in resultados.items():
        print(f"\nüìÑ Documento: {doc}")
        for i, paragrafo in enumerate(parags, 1):
            print(f"\nüîπ Par√°grafo {i}: {paragrafo[:500]}...")

